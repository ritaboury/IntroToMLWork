{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"+3\"><b>Non-Linear Models and Validation Metrics</b></font>"
      ],
      "metadata": {
        "id": "CNDKREiQRJJX"
      },
      "id": "CNDKREiQRJJX"
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Blue'>\n",
        "In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf275ca7",
      "metadata": {
        "id": "cf275ca7"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b67a661",
      "metadata": {
        "id": "2b67a661"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee2d2c3",
      "metadata": {
        "id": "5ee2d2c3"
      },
      "source": [
        "# **Part 1: Regression (14.5 marks)**\n",
        "\n",
        "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8219f163",
      "metadata": {
        "id": "8219f163"
      },
      "source": [
        "## **Step 1:** Data Input (0.5 marks)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
        "\n",
        "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af8bd32",
      "metadata": {
        "id": "2af8bd32"
      },
      "outputs": [],
      "source": [
        "# Import concrete dataset from yellowbrick library\n",
        "from yellowbrick.datasets import load_concrete\n",
        "\n",
        "# Load concrete dataset into the feature matrix X and target vector y\n",
        "X, y = load_concrete()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42fea4cc",
      "metadata": {
        "id": "42fea4cc"
      },
      "source": [
        "## **Step 2:** Data Processing (0 marks)\n",
        "\n",
        "Data processing was completed in the previous assignment. No need to repeat here.\n",
        "\n",
        "<font color='red'>\n",
        "This is just for your information and no action is required from you for this step.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a245d00",
      "metadata": {
        "id": "2a245d00"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model (0.5 marks)\n",
        "\n",
        "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
        "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
        "3. Implement each machine learning model with `X` and `y`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f994e31",
      "metadata": {
        "id": "3f994e31"
      },
      "source": [
        "## **Step 4:** Validate Model (0.5 marks)\n",
        "\n",
        "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fc3f7a8",
      "metadata": {
        "id": "5fc3f7a8"
      },
      "source": [
        "## **Step 5:** Visualize Results (3 marks)\n",
        "\n",
        "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
        "2. Add the accuracy results to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc93a78",
      "metadata": {
        "id": "fdc93a78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8493fe0-0a80-4967-8d65-d79072df1862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Training Accuracy Validation Accuracy\n",
            "DT         47.918561          163.087775\n",
            "RF         32.055432          156.404972\n",
            "GB           3.73927           99.360259\n"
          ]
        }
      ],
      "source": [
        "# ADD YOUR CODE HERE FOR STEPS 3-5\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Instantiate the three models\n",
        "models = {\n",
        "    'DT': DecisionTreeRegressor(max_depth = 5, random_state = 0),\n",
        "    'RF': RandomForestRegressor(max_depth = 5, random_state = 0),\n",
        "    'GB': GradientBoostingRegressor(max_depth = 5, random_state = 0)\n",
        "}\n",
        "\n",
        "# Initialize results DataFrame\n",
        "results = pd.DataFrame(columns=['Training Accuracy', 'Validation Accuracy'], index = models.keys())\n",
        "\n",
        "# Used hint to put code in a loop\n",
        "for name, model in models.items():\n",
        "  # Calculate accuracies with cross-validation\n",
        "  cv_results = cross_validate(model, X, y, scoring = 'neg_mean_squared_error', return_train_score = True)\n",
        "  training_accuracy = -1 * cv_results['train_score'].mean()\n",
        "  validation_accuracy = -1 * cv_results['test_score'].mean()\n",
        "  # Add accuracy results to DataFrame\n",
        "  results.loc[name, 'Training Accuracy'] = training_accuracy\n",
        "  results.loc[name, 'Validation Accuracy'] = validation_accuracy\n",
        "\n",
        "# Print results\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31715a9d",
      "metadata": {
        "id": "31715a9d"
      },
      "source": [
        "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`.\n",
        "\n",
        "<font color='red'>\n",
        "Due to the similarity of this to the main part of step 5, this part is 0.5 and the main part of step 5 is 2.5 of the total 3 points for this step.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83539f47",
      "metadata": {
        "id": "83539f47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f48c64-649c-434c-e7b1-a3b3aa66b3d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Training Accuracy Validation Accuracy\n",
            "DT          0.822887             0.17621\n",
            "RF          0.881221            0.173748\n",
            "GB          0.986436            0.473701\n"
          ]
        }
      ],
      "source": [
        "# This would be similar to the main step, the main difference is the scoring.\n",
        "\n",
        "# Initialize results DataFrame\n",
        "results2 = pd.DataFrame(columns=['Training Accuracy', 'Validation Accuracy'], index = models.keys())\n",
        "\n",
        "# Used hint to put code in a loop\n",
        "for name, model in models.items():\n",
        "  # Calculate accuracies with cross-validation\n",
        "  cv_results = cross_validate(model, X, y, scoring = 'r2', return_train_score = True)\n",
        "  training_accuracy = cv_results['train_score'].mean()\n",
        "  validation_accuracy = cv_results['test_score'].mean()\n",
        "  # Add accuracy results to DataFrame\n",
        "  results2.loc[name, 'Training Accuracy'] = training_accuracy\n",
        "  results2.loc[name, 'Validation Accuracy'] = validation_accuracy\n",
        "\n",
        "# Print results\n",
        "print(results2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5257a98",
      "metadata": {
        "id": "a5257a98"
      },
      "source": [
        "## Questions (6 marks)\n",
        "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
        "1. Out of the models you tested, which model would you select for this dataset and why?\n",
        "1. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Green'><b>\n",
        "1. The linear model in the previous assignment gave high MSE in both the training and validation accuracies (110 and 96 respectively), where its training MSE was much higher than all the non-linear models used above (about 48, 32, and 4 respectively for DT, RF, and GB); however, the validation MSE for the linear model was lower than the models created above (about 163, 156, and 99 respectively).\n",
        "In addition, the R2 scores for the linear model were low (0.61 and 0.64 respectively for the training and validation sets), depicting a lower training accuracy than all the non-linear models above (0.82, 0.88, and 0.99 respectively), but a higher validation accuracy than the models above as well (0.18, 0.17, and 0.47 respectively). Ultimately, even though the linear model has less desirable training accuracies than the models shown above, there is a much smaller gap between its training and validation results which allows for better validation accuracies and much less overfitting than the non-linear models. This can be considered more valuable.\n",
        "2. Out of the three models tested above, I would select the Gradient Boosting Machine regression model because it gave training and validation MSE values of 3.7 and 99.4 respectively, and R2 scores of 0.99 and 0.47 respectively. The MSE values for this model were lower than all the others, which indicates that the predicted values are much closer to the actual values in the data. The R2 scores are much closer to 1 compared to the other models, which supports the fact that the model is capturing the majority of the variance in the data better than the others; however, it is clear that all the models are significantly overfitting the data, proven by the drastic difference between the training and validation accuracies, so either the hyperparameters should be tuned or a different model should be considered.\n",
        "3. Improving the accuracy of the tree-based models can involve pre-pruning by adjusting parameters to prevent overfitting depending on the model. Below, I have listed two suggestions per model:\n",
        "  - Decision Tree: The validation R2 score is much lower than the training R2 score which indicates overfitting, therefore I would suggest:\n",
        "  1. Decrease max_leaf_nodes parameter\n",
        "  2. Increase min_samples_leaf parameter\n",
        "  - Random Forest: The validation R2 score is still much lower than the training R2 score which indicates some overfitting, therefore I would suggest:\n",
        "  1. Increase n_estimators\n",
        "  2. Decrease max_features\n",
        "  - GB: The validation R2 score is still much lower than the training R2 score which indicates some overfitting, therefore I would suggest:\n",
        "  1. Lower n_estimators (opposite to random forests, increasing n_estimators in gradient boosting increases complexity and can lead to overfitting)\n",
        "  2. Decrease learning_rate\n",
        "</b></font>"
      ],
      "metadata": {
        "id": "2PRnpiFjVDzv"
      },
      "id": "2PRnpiFjVDzv"
    },
    {
      "cell_type": "markdown",
      "id": "37b238f4",
      "metadata": {
        "id": "37b238f4"
      },
      "source": [
        "## Process Description (4 marks)\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93097bfe",
      "metadata": {
        "id": "93097bfe"
      },
      "source": [
        "<font color='Green'><b>DESCRIBE YOUR PROCESS HERE</b></font>\n",
        "\n",
        "> I referenced a few sources to come up with my code including the link provided to be able to use the load_concrete() function, as well as notes given in labs 1, 2, 3, and 4 to be able to import the appropriate libraries and use the DecisionTreeRegressor and RandomForestRegressor functions. I referenced the sci-kit learn documentation to help me verify my use of the GradientBoostingRegressor function since it was not used in the labs (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html). I also made sure to properly read the instructions given to create the models and use the cross_validate function, but I also used the sci-kit learn documentation to help me better understand the details of cross_validate (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html). I also referenced my previous assignment.\n",
        "\n",
        "> I completed the steps in the order provided because I believed it made the most sense.\n",
        "\n",
        "> I did not use generative AI for this section.\n",
        "\n",
        "> One challenge I faced was initially trying to understand how to use the cross_validate function, as I had initially split my data set into training and test sets using train_test_split; however, after looking through the documentation, I realized that this was unnecessary since the cross_validate function would split the data on its own. Overall, referencing the documentation and notes helped me be successful."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c6de86",
      "metadata": {
        "id": "f7c6de86"
      },
      "source": [
        "# **Part 2: Classification (17.5 marks)**\n",
        "\n",
        "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9d33a8",
      "metadata": {
        "id": "5f9d33a8"
      },
      "source": [
        "## **Step 1:** Data Input (2 marks)\n",
        "\n",
        "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
        "\n",
        "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset\n",
        "\n",
        "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
        "\n",
        "Print the size and type of `X` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33583c67",
      "metadata": {
        "id": "33583c67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7d80d3-8d7f-45da-f923-b3b65ce69c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of X: 2314\n",
            "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
            "Size of y: 178\n",
            "Type of y: <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# Define column headers based on website description\n",
        "column_names = ['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium',\n",
        "                'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
        "                'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
        "\n",
        "# Import wine dataset\n",
        "wine = pd.read_csv('wine.data', names = column_names)\n",
        "\n",
        "# Class column represents the target vector\n",
        "# Split data into feature matrix X and target vector y\n",
        "X = wine.drop('Class', axis = 1)\n",
        "y = wine['Class']\n",
        "\n",
        "# Print size and type of X and y\n",
        "print(\"Size of X:\", X.size)\n",
        "print(\"Type of X:\", type(X))\n",
        "print(\"Size of y:\", y.size)\n",
        "print(\"Type of y:\", type(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156db208",
      "metadata": {
        "id": "156db208"
      },
      "source": [
        "## **Step 2:** Data Processing (1.5 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a28af110",
      "metadata": {
        "id": "a28af110"
      },
      "source": [
        "Print the first five rows of the dataset to inspect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea266921",
      "metadata": {
        "id": "ea266921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "9d6f0b66-4b8c-4eab-9057-2e5024411215"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  Total phenols  \\\n",
              "0    14.23        1.71  2.43               15.6        127           2.80   \n",
              "1    13.20        1.78  2.14               11.2        100           2.65   \n",
              "2    13.16        2.36  2.67               18.6        101           2.80   \n",
              "3    14.37        1.95  2.50               16.8        113           3.85   \n",
              "4    13.24        2.59  2.87               21.0        118           2.80   \n",
              "\n",
              "   Flavanoids  Nonflavanoid phenols  Proanthocyanins  Color intensity   Hue  \\\n",
              "0        3.06                  0.28             2.29             5.64  1.04   \n",
              "1        2.76                  0.26             1.28             4.38  1.05   \n",
              "2        3.24                  0.30             2.81             5.68  1.03   \n",
              "3        3.49                  0.24             2.18             7.80  0.86   \n",
              "4        2.69                  0.39             1.82             4.32  1.04   \n",
              "\n",
              "   OD280/OD315 of diluted wines  Proline  \n",
              "0                          3.92     1065  \n",
              "1                          3.40     1050  \n",
              "2                          3.17     1185  \n",
              "3                          3.45     1480  \n",
              "4                          2.93      735  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf575304-0dba-471a-b4c4-cc605c97485a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Alcalinity of ash</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD280/OD315 of diluted wines</th>\n",
              "      <th>Proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf575304-0dba-471a-b4c4-cc605c97485a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bf575304-0dba-471a-b4c4-cc605c97485a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bf575304-0dba-471a-b4c4-cc605c97485a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-52c81d48-d2ca-47eb-99bb-f7fa1741f7be\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52c81d48-d2ca-47eb-99bb-f7fa1741f7be')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-52c81d48-d2ca-47eb-99bb-f7fa1741f7be button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X",
              "summary": "{\n  \"name\": \"X\",\n  \"rows\": 178,\n  \"fields\": [\n    {\n      \"column\": \"Alcohol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8118265380058575,\n        \"min\": 11.03,\n        \"max\": 14.83,\n        \"num_unique_values\": 126,\n        \"samples\": [\n          11.62,\n          13.64,\n          13.69\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Malic acid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1171460976144627,\n        \"min\": 0.74,\n        \"max\": 5.8,\n        \"num_unique_values\": 133,\n        \"samples\": [\n          1.21,\n          2.83,\n          1.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ash\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27434400906081485,\n        \"min\": 1.36,\n        \"max\": 3.23,\n        \"num_unique_values\": 79,\n        \"samples\": [\n          2.31,\n          2.43,\n          2.52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Alcalinity of ash\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.339563767173505,\n        \"min\": 10.6,\n        \"max\": 30.0,\n        \"num_unique_values\": 63,\n        \"samples\": [\n          25.5,\n          28.5,\n          15.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Magnesium\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 70,\n        \"max\": 162,\n        \"num_unique_values\": 53,\n        \"samples\": [\n          126,\n          85,\n          162\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total phenols\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6258510488339893,\n        \"min\": 0.98,\n        \"max\": 3.88,\n        \"num_unique_values\": 97,\n        \"samples\": [\n          1.68,\n          2.11,\n          1.35\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Flavanoids\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9988586850169467,\n        \"min\": 0.34,\n        \"max\": 5.08,\n        \"num_unique_values\": 132,\n        \"samples\": [\n          3.18,\n          2.5,\n          3.17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Nonflavanoid phenols\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12445334029667937,\n        \"min\": 0.13,\n        \"max\": 0.66,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          0.58,\n          0.41,\n          0.39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Proanthocyanins\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5723588626747613,\n        \"min\": 0.41,\n        \"max\": 3.58,\n        \"num_unique_values\": 101,\n        \"samples\": [\n          0.75,\n          1.77,\n          1.42\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color intensity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.318285871822413,\n        \"min\": 1.28,\n        \"max\": 13.0,\n        \"num_unique_values\": 132,\n        \"samples\": [\n          2.95,\n          3.3,\n          5.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hue\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22857156582982338,\n        \"min\": 0.48,\n        \"max\": 1.71,\n        \"num_unique_values\": 78,\n        \"samples\": [\n          1.22,\n          1.04,\n          1.45\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"OD280/OD315 of diluted wines\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7099904287650504,\n        \"min\": 1.27,\n        \"max\": 4.0,\n        \"num_unique_values\": 122,\n        \"samples\": [\n          4.0,\n          1.82,\n          1.59\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Proline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 314,\n        \"min\": 278,\n        \"max\": 1680,\n        \"num_unique_values\": 121,\n        \"samples\": [\n          1375,\n          1270,\n          735\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Print first five rows of dataset\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "834fc8fe",
      "metadata": {
        "id": "834fc8fe"
      },
      "source": [
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c6e9dc",
      "metadata": {
        "id": "97c6e9dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b65bb2-bb77-444e-b009-4d68f6a85936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alcohol                         False\n",
            "Malic acid                      False\n",
            "Ash                             False\n",
            "Alcalinity of ash               False\n",
            "Magnesium                       False\n",
            "Total phenols                   False\n",
            "Flavanoids                      False\n",
            "Nonflavanoid phenols            False\n",
            "Proanthocyanins                 False\n",
            "Color intensity                 False\n",
            "Hue                             False\n",
            "OD280/OD315 of diluted wines    False\n",
            "Proline                         False\n",
            "dtype: bool\n"
          ]
        }
      ],
      "source": [
        "# Check if there are any missing values and fill them in if necessary\n",
        "missing_values = X.isnull().any()\n",
        "print(missing_values)\n",
        "\n",
        "# All values are False (no values are missing from any column) so no method to fill-in is necessary\n",
        "# If a number was missing, it could be filled with the mean value using the line below\n",
        "# X.fillna(X.mean(), inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "070956af",
      "metadata": {
        "id": "070956af"
      },
      "source": [
        "How many samples do we have of each type of wine?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b37a6fd9",
      "metadata": {
        "id": "b37a6fd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4400ed-0028-41f5-8dbe-c679255dc0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2    71\n",
            "1    59\n",
            "3    48\n",
            "Name: Class, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Find number of samples for each type of wine (class in target vector y)\n",
        "wine_type_counts = y.value_counts()\n",
        "print(wine_type_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e6c46f",
      "metadata": {
        "id": "70e6c46f"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model\n",
        "\n",
        "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
        "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
        "3. Implement the machine learning model with `X` and `y`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0870b0d2",
      "metadata": {
        "id": "0870b0d2"
      },
      "source": [
        "## **Step 4:** Validate Model\n",
        "\n",
        "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb0bbd83",
      "metadata": {
        "id": "bb0bbd83"
      },
      "source": [
        "## **Step 5:** Visualize Results (4 marks)\n",
        "\n",
        "<font color='red'>\n",
        "There is no individual mark for Steps 3 and 4 and those grades are included within the four points.\n",
        "\n",
        "</font>\n",
        "\n",
        "### **Step 5.1:** Compare Models (2 out of total 4 marks)\n",
        "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
        "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be4b5c0a",
      "metadata": {
        "id": "be4b5c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "decf7c3f-48d4-4f59-8b30-32666cd1718d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Training Accuracy Validation Accuracy\n",
            "SVC          0.703743            0.663492\n",
            "DTC          0.974756            0.893175\n"
          ]
        }
      ],
      "source": [
        "# ADD YOUR CODE HERE FOR STEPS 3-5\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Instantiate the two models\n",
        "models = {\n",
        "    'SVC': SVC(random_state = 0),\n",
        "    'DTC': DecisionTreeClassifier(max_depth = 3, random_state = 0)\n",
        "}\n",
        "\n",
        "# Initialize results DataFrame\n",
        "results = pd.DataFrame(columns=['Training Accuracy', 'Validation Accuracy'])\n",
        "\n",
        "# Used hint to put code in a loop\n",
        "for name, model in models.items():\n",
        "  # Calculate accuracies with cross-validation\n",
        "  cv_results = cross_validate(model, X, y, scoring = 'accuracy', return_train_score = True)\n",
        "  training_accuracy = cv_results['train_score'].mean()\n",
        "  validation_accuracy = cv_results['test_score'].mean()\n",
        "  # Add accuracy results to DataFrame\n",
        "  results.loc[name, 'Training Accuracy'] = training_accuracy\n",
        "  results.loc[name, 'Validation Accuracy'] = validation_accuracy\n",
        "\n",
        "# Print results\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2e17878",
      "metadata": {
        "id": "f2e17878"
      },
      "source": [
        "### **Step 5.2:** Visualize Classification Errors  (2 out of total 4 marks)\n",
        "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b091a4",
      "metadata": {
        "id": "44b091a4"
      },
      "outputs": [],
      "source": [
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Implement best model: DTC\n",
        "best_model = DecisionTreeClassifier(max_depth = 3, random_state = 0)\n",
        "best_model.fit(X_train, y_train)\n",
        "y_pred = best_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09d21b59",
      "metadata": {
        "id": "09d21b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "36578544-2ad9-49cd-ea9b-4a8b75165ba7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Print confusion matrix using a heatmap\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot = True, cmap = 'Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for DTC Model')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef95947",
      "metadata": {
        "id": "5ef95947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3bf6fe-a93f-4431-8e1e-873de7c15422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.93      1.00      0.97        14\n",
            "           2       1.00      0.94      0.97        16\n",
            "           3       1.00      1.00      1.00         6\n",
            "\n",
            "    accuracy                           0.97        36\n",
            "   macro avg       0.98      0.98      0.98        36\n",
            "weighted avg       0.97      0.97      0.97        36\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print classification report\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf319621",
      "metadata": {
        "id": "bf319621"
      },
      "source": [
        "## Questions (6 marks)\n",
        "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
        "1. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
        "1. How many samples were incorrectly classified in step 5.2?\n",
        "1. In this case, is maximizing precision or recall more important? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font color='Green'><b>\n",
        "1. The training and validation accuracies were much lower with the SVC model compared to the DecisionTreeClassifier (DTC). The SVC accuracies were about 0.70 and 0.66 respectively for the training and validation datasets, while the DTC accuracies were about 0.97 and 0.89 respectively. This shows that the DTC model fits the data in the training set and predicts the data in the test set much more accurately.\n",
        "2. Two reasons why the support vector machines model did not work as well as the tree-based model could be feature scaling and hyperparameter tuning. First of all, SVCs require all the features to vary on a similar scale; however, the wine dataset shows different scales depending on the feature and the data was not preprocessed to account for this. Decision trees are invariant to scaling and do not require preprocessing, so this reason would explain why SVC did not work as well as DTC in this situation. Second of all, SVMs usually require a careful tuning of parameters; however, the SVC model created above was not instantiated with any tuned hyperparameters such as C or gamma. On the other hand, the DTC model created above adjusts the hyperparameter max_depth to reduce its overfitting, so this could have also contributed to the results.\n",
        "3. Based on the confusion matrix for the DTC, only one sample in the test dataset was incorrectly classified, since the predicted label of class 0 did not match the true label of class 1.\n",
        "4. In this case, I believe that maximizing precision would be considered more important because it involves minimizing false positives as opposed to false negatives. Misclassifying a wine, especially a bad wine as premium (false positive), could damage a company's reputation and cause them to lose consumer trust, depicting the value of high precision. On the other hand, recall would want to be maximized if there was a huge cost of failing to identify a specific class of wine like premium for example (false negative), even if it means some lower-quality wines slip through. Overall, while both precision and recall are important, I believe that maximizing precision is particularly critical in the wine industry to maintain a brand's reputation and ensure customer satisfaction.\n",
        "</b></font>"
      ],
      "metadata": {
        "id": "1FQstcwnXXng"
      },
      "id": "1FQstcwnXXng"
    },
    {
      "cell_type": "markdown",
      "id": "664ff8ae",
      "metadata": {
        "id": "664ff8ae"
      },
      "source": [
        "## Process Description (4 marks)\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e837da",
      "metadata": {
        "id": "d0e837da"
      },
      "source": [
        "<font color='Green'><b>DESCRIBE YOUR PROCESS HERE</b></font>\n",
        "\n",
        "> I referenced a few sources to come up with my code including the link provided to be able to download the wine dataset, as well as notes given in labs 1, 2, 3, and 4 to be able to import the appropriate libraries and use the DecisionTreeClassifier and SVC functions (variants of their similar regressor functions). I also made sure to properly read the instructions given to create the models and calculate the accuracies. I was able to reference the confusion matrix code from lab 2, but I had to reference sci-kit documentation for the classification report (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html). I also referenced my previous assignment.\n",
        "\n",
        "> I completed the steps in the order provided because I believed it made the most sense.\n",
        "\n",
        "> I did not use generative AI for this section.\n",
        "\n",
        "> One challenge I faced was initially trying to understand how to create the confusion matrix and classification report; however, I was able to figure it out using the resources explained above. Overall, referencing the documentation and lab notes helped me be successful."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd7358d",
      "metadata": {
        "id": "4cd7358d"
      },
      "source": [
        "# **Part 3: Observations/Interpretation (3 marks)**\n",
        "\n",
        "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Green'><b>\n",
        "A huge observation that I noticed in Part 1 was the large presence of overfitting in tree-based models. All 3 models, including the Decision Tree, Random Forest and Gradient Boosting Machines regression models, depicted desirable training accuracies with non-desirable validation accuracies that were much worse, implying overfitting. Specifically, all 3 training R2 scores ranged from 0.82 to 0.99, while the validation R2 scores ranged from 0.17 to 0.47 which shows a dramatic difference. This pattern emphasizes the importance of parameter tuning to reduce overfitting in tree-based models as discussed in lectures.<br> <br>\n",
        "\n",
        "The main observation that I had from Part 2 would be the importance of feature scaling to SVM models as opposed to Decision Tree models. It is clear that a huge reason why the support vector machines model did not work as well as the tree-based model was because of feature scaling, as it was discussed as a weakness of SVM models without preprocessing. This was evident in the results as the SVC accuracies were about 0.70 and 0.66 respectively for the training and validation datasets, while the DTC accuracies were about 0.97 and 0.89 respectively.\n",
        "</b></font>"
      ],
      "metadata": {
        "id": "F3ifv218XL62"
      },
      "id": "F3ifv218XL62"
    },
    {
      "cell_type": "markdown",
      "id": "cd97b6ac",
      "metadata": {
        "id": "cd97b6ac"
      },
      "source": [
        "## **Part 4:** Reflection (2 marks)\n",
        "Include a sentence or two about:\n",
        "- what you liked or disliked,\n",
        "- found interesting, confusing, challangeing, motivating\n",
        "while working on this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Green'><b>\n",
        "I really liked being able to calculate and compare different regression and classification metrics for real-life datasets in order to better understand the strengths and weaknesses of each when trying to create a model that is best suited for a specific dataset. I found it interesting that the tree-based models overfit the data so drastically, but it was challenging at times to understand how the hyperparameters could be tuned to improve the results.\n",
        "</b></font>"
      ],
      "metadata": {
        "id": "tDFYc89YXQGJ"
      },
      "id": "tDFYc89YXQGJ"
    },
    {
      "cell_type": "markdown",
      "id": "fa21e53b",
      "metadata": {
        "id": "fa21e53b"
      },
      "source": [
        "## **Part 5:** Bonus Question (3 marks)\n",
        "\n",
        "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
        "\n",
        "Is `LinearSVC` a good fit for this dataset? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30fea72e",
      "metadata": {
        "id": "30fea72e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0a6287-029f-4555-9130-b224e17706d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Training Accuracy Validation Accuracy\n",
            "LinearSVC          0.880725            0.921746\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Ignore ConvergenceWarning\n",
        "warnings.simplefilter('ignore', ConvergenceWarning)\n",
        "\n",
        "# Instantiate the models\n",
        "linearSVC_model = LinearSVC(max_iter = 5000, random_state = 0)\n",
        "\n",
        "# Initialize results DataFrame\n",
        "results = pd.DataFrame(columns=['Training Accuracy', 'Validation Accuracy'])\n",
        "\n",
        "# Calculate accuracies with cross-validation\n",
        "cv_results = cross_validate(linearSVC_model, X, y, scoring = 'accuracy', return_train_score = True)\n",
        "training_accuracy = cv_results['train_score'].mean()\n",
        "validation_accuracy = cv_results['test_score'].mean()\n",
        "\n",
        "# Add accuracy results to DataFrame\n",
        "results.loc['LinearSVC', 'Training Accuracy'] = training_accuracy\n",
        "results.loc['LinearSVC', 'Validation Accuracy'] = validation_accuracy\n",
        "\n",
        "# Print results\n",
        "print(results)\n",
        "\n",
        "# Reset warnings to default\n",
        "warnings.simplefilter('default', ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aabc68a4",
      "metadata": {
        "id": "aabc68a4"
      },
      "source": [
        "> Using LinearSVC clearly improved the accuracy scores of the dataset in Part 2 compared to the SVC model (0.70 and 0.66 respectively for the training and validation sets). This could imply that the data was more linearly separable than complex because LinearSVC specifically looks for a linear decision boundary, while the SVC model used the default RBF kernel which is more suited to look for complexities.\n",
        "\n",
        "> On the other hand, the LinearSVC model gave a lower training accuracy than the Decision Tree Classifier model (0.88 < 0.97), but a higher validation accuracy (0.92 > 0.89), which insinuates that DTC was able to fit the training data better, but LinearSVC predicted unseen values more accurately. DTC models are more flexible in capturing relationships between features, which would explain the higher training accuracy scores; however, decision trees are also prone to overfitting which supports the DTC model's lower validation accuracy.\n",
        "\n",
        "> Overall, LinearSVC is a good fit for this dataset because it allows for high accuracy scores, especially a high validation accuracy which is the most important for predicting future values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241c3b12",
      "metadata": {
        "id": "241c3b12"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}